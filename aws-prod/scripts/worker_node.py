# import os
# import time
# import requests
# import grpc
# from concurrent import futures

# import pandas as pd
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import accuracy_score

# # Generated by protoc
# import ml_task_pb2
# import ml_task_pb2_grpc

# # We'll store dataset paths here so we don't re-download multiple times.
# DOWNLOADED_DATASETS = {}

# def download_file(dataset_url, local_path):
#     """
#     Download from dataset_url if local_path not present.
#     """
#     if os.path.exists(local_path):
#         return  # Already downloaded
#     resp = requests.get(dataset_url)
#     resp.raise_for_status()
#     with open(local_path, 'wb') as f:
#         f.write(resp.content)

# def load_dataset(local_path):
#     """
#     Load CSV from local_path. 
#     For simplicity, assume last column is the target label.
#     """
#     df = pd.read_csv(local_path)
#     X = df.iloc[:, :-1].values
#     y = df.iloc[:, -1].values
#     return X, y

# class MLTaskService(ml_task_pb2_grpc.MLTaskServiceServicer):
#     def __init__(self, worker_id):
#         self.worker_id = worker_id

#     def DownloadDataset(self, request, context):
#         dataset_id = request.dataset_id  # Could be a direct URL
#         local_path = f"/tmp/{os.path.basename(dataset_id) or 'dataset.csv'}"

#         try:
#             if dataset_id not in DOWNLOADED_DATASETS:
#                 download_file(dataset_id, local_path)
#                 DOWNLOADED_DATASETS[dataset_id] = local_path
#                 return ml_task_pb2.DownloadResponse(
#                     success=True,
#                     message="Dataset downloaded"
#                 )
#             else:
#                 return ml_task_pb2.DownloadResponse(
#                     success=True,
#                     message="Dataset already cached"
#                 )
#         except Exception as e:
#             return ml_task_pb2.DownloadResponse(success=False, message=str(e))

#     def ExecuteTask(self, request, context):
#         from sklearn.model_selection import train_test_split
#         from sklearn.neighbors import KNeighborsClassifier  # For KNN example
#         from sklearn.impute import SimpleImputer  # For imputing missing values

#         dataset_id = request.dataset_id
#         algorithm = request.algorithm
#         hyperparams = request.hyperparameters

#     # Check that the dataset is available
#         if dataset_id not in DOWNLOADED_DATASETS:
#          return ml_task_pb2.TaskResponse(
#             worker_id=f"worker{self.worker_id}",
#             result="Error: Dataset not downloaded yet",
#             execution_time=0.0
#         )

#         local_path = DOWNLOADED_DATASETS[dataset_id]
#         X, y = load_dataset(local_path)

#         # Impute missing values in X (if any)
#         imputer = SimpleImputer(strategy='mean')
#         X = imputer.fit_transform(X)

#         # Split into training and testing sets (70% train, 30% test)
#         X_train, X_test, y_train, y_test = train_test_split(
#             X, y, test_size=0.3, random_state=42
#         )

#         start_time = time.time()

#         if algorithm == "RandomForest":
            
#             from sklearn.ensemble import RandomForestClassifier
#             n_estimators = int(hyperparams.get("n_estimators", "10"))
#             max_depth = int(hyperparams.get("max_depth", "5"))
#             model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)
#         elif algorithm == "KNN":
#              n_neighbors = int(hyperparams.get("n_neighbors", "5"))
#              model = KNeighborsClassifier(n_neighbors=n_neighbors)
#         else:
#             return ml_task_pb2.TaskResponse(
#             worker_id=f"worker{self.worker_id}",
#             result=f"Unsupported algorithm: {algorithm}",
#             execution_time=0.0
#         )

#         # Train the model on the training data
#         model.fit(X_train, y_train)
#         # Evaluate on the test data
#         y_pred = model.predict(X_test)
#         from sklearn.metrics import accuracy_score
#         accuracy = accuracy_score(y_test, y_pred)
#         result_str = f"{algorithm} trained with test accuracy: {accuracy:.2f}"

#         exec_time = time.time() - start_time

#         return ml_task_pb2.TaskResponse(
#         worker_id=f"worker{self.worker_id}",
#         result=result_str,
#         execution_time=exec_time
#     )


# def serve():
#     worker_id = os.environ.get("WORKER_ID", "X")
#     server = grpc.server(futures.ThreadPoolExecutor(max_workers=4))
#     ml_task_pb2_grpc.add_MLTaskServiceServicer_to_server(MLTaskService(worker_id), server)
#     server.add_insecure_port("[::]:50051")
#     print(f"Worker node {worker_id} started, listening on 50051...")
#     server.start()

#     try:
#         while True:
#             time.sleep(3600)
#     except KeyboardInterrupt:
#         server.stop(0)

# if __name__ == "__main__":
#     serve()

# import os
# import time
# import json
# import requests
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.impute import SimpleImputer 
# from sklearn.metrics import accuracy_score
# from kafka import KafkaConsumer, KafkaProducer

# import logging

# logging.basicConfig(level=logging.INFO)

# KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")

# DOWNLOADED_DATASETS = {}

# def download_file(dataset_url, local_path):
#     if os.path.exists(local_path):
#         return
#     resp = requests.get(dataset_url)
#     resp.raise_for_status()
#     with open(local_path, 'wb') as f:
#         f.write(resp.content)

# def load_dataset(local_path):
#     df = pd.read_csv(local_path)
#     X = df.iloc[:, :-1].values
#     y = df.iloc[:, -1].values
#     return X, y

# def process_task(task):
#     worker_id = os.environ.get("WORKER_ID", "X")
#     dataset_id = task["dataset_id"]
#     algorithm = task["algorithm"]
#     hyperparams = task["hyperparameters"]

#     # Download dataset if not cached
#     local_path = f"/tmp/{os.path.basename(dataset_id) or 'dataset.csv'}"
#     if dataset_id not in DOWNLOADED_DATASETS:
#         try:
#             download_file(dataset_id, local_path)
#             DOWNLOADED_DATASETS[dataset_id] = local_path
#         except Exception as e:
#             return {
#                 "worker_id": f"worker{worker_id}",
#                 "request_id": task["request_id"],
#                 "error": str(e)
#             }

#     # Load and train
#     X, y = load_dataset(DOWNLOADED_DATASETS[dataset_id])
#     imputer = SimpleImputer(strategy='mean')  # fill NaNs with column mean
#     X = imputer.fit_transform(X)
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
#     start_time = time.time()

#     if algorithm == "RandomForest":
#         n_estimators = int(hyperparams.get("n_estimators", "10"))
#         max_depth = int(hyperparams.get("max_depth", "5"))
#         model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)
#     elif algorithm == "KNN":
#         n_neighbors = int(hyperparams.get("n_neighbors", "5"))
#         model = KNeighborsClassifier(n_neighbors=n_neighbors)
#     else:
#         return {
#             "worker_id": f"worker{worker_id}",
#             "request_id": task["request_id"],
#             "error": f"Unsupported algorithm: {algorithm}"
#         }

#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_test)
#     accuracy = accuracy_score(y_test, y_pred)
#     exec_time = time.time() - start_time
#     accuracy_rounded = round(accuracy, 2)

#     return {
#         "worker_id": f"worker{worker_id}",
#         "request_id": task["request_id"],
#         "accuracy": accuracy_rounded,
#         "execution_time": exec_time,
#         "hyperparameters": hyperparams
#     }

# def kafka_task_loop():
#     consumer = KafkaConsumer(
#         "task_requests",
#         bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#         value_deserializer=lambda m: json.loads(m.decode("utf-8")),
#         auto_offset_reset="earliest",
#         group_id=f"worker-group-{os.environ.get('WORKER_ID', 'X')}"
#     )
#     producer = KafkaProducer(
#         bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#         value_serializer=lambda v: json.dumps(v).encode("utf-8")
#     )
#     for message in consumer:
#         task = message.value
#         result = process_task(task)
#         # Publish result to task_responses
#         producer.send("task_responses", result)
#         producer.flush()

# if __name__ == "__main__":
#     kafka_task_loop()



# import os
# import time
# import json
# import requests
# import pandas as pd
# import logging
# from sklearn.model_selection import train_test_split
# from xgboost import XGBClassifier
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.metrics import accuracy_score
# from sklearn.impute import SimpleImputer
# from kafka import KafkaConsumer, KafkaProducer

# # Set up logging
# logging.basicConfig(level=logging.INFO)

# # Get Kafka bootstrap servers from environment (default to "kafka:9092")
# KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")

# # Cache to store downloaded dataset file paths
# DOWNLOADED_DATASETS = {}

# def download_file(dataset_url, local_path):
#     """
#     Download the dataset from the provided URL in chunks (1 MB per chunk)
#     and save it to local_path.
#     """
#     if os.path.exists(local_path):
#         return
#     logging.info(f"Downloading dataset from {dataset_url}...")
#     with requests.get(dataset_url, stream=True) as r:
#         r.raise_for_status()
#         with open(local_path, 'wb') as f:
#             for chunk in r.iter_content(chunk_size=1024*1024):
#                 f.write(chunk)
#     logging.info(f"Downloaded dataset to {local_path}")

# def load_large_dataset(local_path):
#     """
#     Load a subset of the CSV dataset from local_path.
#     For testing purposes, we sample 50,000 rows.
#     """
#     # If the file is compressed (ends with .gz), use compression
#     compression = "gzip" if local_path.endswith(".gz") else None
#     df = pd.read_csv(local_path, compression=compression, header=None)
#     df = df.sample(n=50000)  # Sample 50,000 rows for testing
#     X = df.iloc[:, :-1].values
#     y = df.iloc[:, -1].values
#     return X, y

# def process_task(task):
#     """
#     Process a task message: download dataset if needed, load and process data,
#     train a model using one of the algorithms (XGBoost, RandomForest, KNN),
#     and return the result.
#     """
#     worker_id = os.environ.get("WORKER_ID", "X")
#     # Here, "dataset_id" is actually a URL to a CSV file.
#     dataset_url = task["dataset_id"]
#     algorithm = task["algorithm"]
#     hyperparams = task["hyperparameters"]
#     request_id = task["request_id"]

#     # Set local path based on the URL's basename
#     local_path = f"/tmp/{os.path.basename(dataset_url)}"
#     if not os.path.exists(local_path):
#         try:
#             download_file(dataset_url, local_path)
#             DOWNLOADED_DATASETS[dataset_url] = local_path
#         except Exception as e:
#             return {
#                 "worker_id": f"worker{worker_id}",
#                 "request_id": request_id,
#                 "error": str(e)
#             }

#     # Load a subset of the dataset
#     X, y = load_large_dataset(local_path)

#     # Impute missing values (if any)
#     imputer = SimpleImputer(strategy='mean')
#     X = imputer.fit_transform(X)

#     # Split data into training and testing sets (70/30 split)
#     X_train, X_test, y_train, y_test = train_test_split(
#         X, y, test_size=0.3, random_state=42
#     )
    
#     start_time = time.time()

#     # Choose model based on algorithm specified
#     if algorithm == "XGBoost":
#         model = XGBClassifier(
#             n_estimators=int(hyperparams.get("n_estimators", "100")),
#             max_depth=int(hyperparams.get("max_depth", "5")),
#             learning_rate=float(hyperparams.get("learning_rate", "0.1"))
#         )
#     elif algorithm == "RandomForest":
#         model = RandomForestClassifier(
#             n_estimators=int(hyperparams.get("n_estimators", "100")),
#             max_depth=int(hyperparams.get("max_depth", "5"))
#         )
#     elif algorithm == "KNN":
#         model = KNeighborsClassifier(
#             n_neighbors=int(hyperparams.get("n_neighbors", "5"))
#         )
#     else:
#         return {
#             "worker_id": f"worker{worker_id}",
#             "request_id": request_id,
#             "error": f"Unsupported algorithm: {algorithm}"
#         }

#     # Train the model and evaluate
#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_test)
#     accuracy = round(accuracy_score(y_test, y_pred), 2)
#     exec_time = round(time.time() - start_time, 2)

#     response = {
#         "worker_id": f"worker{worker_id}",
#         "request_id": request_id,
#         "accuracy": accuracy,
#         "execution_time": exec_time,
#         "hyperparameters": hyperparams
#     }
#     logging.info(f"Worker {worker_id} finished task. Response: {json.dumps(response, indent=2)}")
#     return response

# def kafka_task_loop():
#     """
#     Kafka task loop: each worker consumes messages from 'task_requests'
#     and publishes results to 'task_responses'.
#     """
#     consumer = KafkaConsumer(
#         "task_requests",
#         bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#         value_deserializer=lambda m: json.loads(m.decode("utf-8")),
#         auto_offset_reset="earliest",
#         group_id=f"worker-group-{os.environ.get('WORKER_ID', 'X')}"
#     )
#     producer = KafkaProducer(
#         bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#         value_serializer=lambda v: json.dumps(v).encode("utf-8")
#     )
#     for message in consumer:
#         task = message.value
#         result = process_task(task)
#         producer.send("task_responses", result)
#         producer.flush()

# if __name__ == "__main__":
#     kafka_task_loop()


# import os
# import time
# import json
# import requests
# import pandas as pd
# import logging
# from sklearn.model_selection import train_test_split
# from xgboost import XGBClassifier
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.metrics import accuracy_score
# from sklearn.impute import SimpleImputer
# from kafka import KafkaConsumer, KafkaProducer

# # Import Kaggle API for dataset downloads via Kaggle
# from kaggle.api.kaggle_api_extended import KaggleApi

# # Set up logging
# logging.basicConfig(level=logging.INFO)

# # Kafka configuration
# KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")

# # Cache for downloaded datasets (maps dataset URL to local file path)
# DOWNLOADED_DATASETS = {}

# def download_http_file(dataset_url, local_path):
#     """
#     Download the dataset via HTTP in chunks (1 MB per chunk) and save to local_path.
#     """
#     if os.path.exists(local_path):
#         logging.info(f"File already exists: {local_path}")
#         return local_path
#     logging.info(f"Downloading dataset from {dataset_url} via HTTP...")
#     with requests.get(dataset_url, stream=True) as r:
#         r.raise_for_status()
#         with open(local_path, 'wb') as f:
#             for chunk in r.iter_content(chunk_size=1024*1024):
#                 f.write(chunk)
#     logging.info(f"Downloaded dataset to {local_path}")
#     return local_path

# def download_kaggle_dataset(dataset_url, download_dir):
#     """
#     Download a Kaggle dataset using the Kaggle API.
#     Expects a URL in the form:
#        https://www.kaggle.com/datasets/<owner>/<dataset-name>
#     Downloads and unzips dataset files into download_dir and returns the path
#     to the first CSV file found.
#     """
#     parts = dataset_url.strip().split('/')
#     if len(parts) < 6:
#         raise ValueError("Invalid Kaggle dataset URL format.")
#     dataset_id = parts[-2] + "/" + parts[-1]
#     logging.info(f"Downloading Kaggle dataset {dataset_id} into {download_dir} ...")
#     api = KaggleApi()
#     api.authenticate()
#     api.dataset_download_files(dataset_id, path=download_dir, unzip=True)
#     # Find the first CSV file in the download directory.
#     for file in os.listdir(download_dir):
#         if file.endswith(".csv"):
#             csv_path = os.path.join(download_dir, file)
#             logging.info(f"Found CSV file: {csv_path}")
#             return csv_path
#     raise ValueError("No CSV file found in the Kaggle dataset.")

# def download_file(dataset_url, local_path):
#     """
#     Decide which method to use for downloading based on the URL.
#     If the URL contains 'kaggle.com/datasets', use the Kaggle API.
#     Otherwise, use HTTP download.
#     """
#     if "kaggle.com/datasets" in dataset_url:
#         # Ensure the download directory exists
#         download_dir = os.path.dirname(local_path)
#         os.makedirs(download_dir, exist_ok=True)
#         return download_kaggle_dataset(dataset_url, download_dir)
#     else:
#         return download_http_file(dataset_url, local_path)

# def load_large_dataset(local_path):
#     """
#     Load a subset of the CSV dataset from local_path.
#     For testing, we sample 50,000 rows.
#     Assumes the CSV file has headers.
#     """
#     compression = "gzip" if local_path.endswith(".gz") else None
#     df = pd.read_csv(local_path, compression=compression, header=0)
#     if len(df) > 50000:
#         df = df.sample(n=50000, random_state=42)
#     logging.info(f"Loaded dataset from {local_path} with {len(df)} rows.")
#     # Assume the target column is the last column.
#     X = df.iloc[:, :-1].values
#     y = df.iloc[:, -1].values
#     return X, y

# def process_task(task):
#     """
#     Process an ML task:
#       - Download dataset (via Kaggle API if needed) if not already cached.
#       - Load and sample the data.
#       - Impute missing values.
#       - Split data into train and test sets.
#       - Train a model (XGBoost, RandomForest, or KNN) based on provided hyperparameters.
#       - Return model accuracy and execution time.
#     """
#     worker_id = os.environ.get("WORKER_ID", "X")
#     dataset_url = task["dataset_id"]
#     algorithm = task["algorithm"]
#     hyperparams = task["hyperparameters"]
#     request_id = task["request_id"]

#     # Determine local file path based on URL basename.
#     basename = os.path.basename(dataset_url)
#     if not (basename.endswith(".csv") or basename.endswith(".csv.gz")):
#         basename += ".csv"
#     local_path = f"/tmp/{basename}"
    
#     # Download dataset if not cached.
#     if dataset_url not in DOWNLOADED_DATASETS:
#         try:
#             local_path = download_file(dataset_url, local_path)
#             DOWNLOADED_DATASETS[dataset_url] = local_path
#         except Exception as e:
#             return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": str(e)}
    
#     # Load the dataset
#     try:
#         X, y = load_large_dataset(DOWNLOADED_DATASETS[dataset_url])
#     except Exception as e:
#         return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": f"Error loading dataset: {e}"}
    
#     # Impute missing values
#     imputer = SimpleImputer(strategy='mean')
#     X = imputer.fit_transform(X)
    
#     # Split data (70% train, 30% test)
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
#     start_time = time.time()
    
#     # Choose the model based on the specified algorithm
#     if algorithm == "XGBoost":
#         model = XGBClassifier(
#             n_estimators=int(hyperparams.get("n_estimators", "100")),
#             max_depth=int(hyperparams.get("max_depth", "5")),
#             learning_rate=float(hyperparams.get("learning_rate", "0.1"))
#         )
#     elif algorithm == "RandomForest":
#         model = RandomForestClassifier(
#             n_estimators=int(hyperparams.get("n_estimators", "100")),
#             max_depth=int(hyperparams.get("max_depth", "5"))
#         )
#     elif algorithm == "KNN":
#         model = KNeighborsClassifier(
#             n_neighbors=int(hyperparams.get("n_neighbors", "5"))
#         )
#     else:
#         return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": f"Unsupported algorithm: {algorithm}"}
    
#     try:
#         model.fit(X_train, y_train)
#     except Exception as e:
#         logging.error(f"Error during model.fit: {e}")
#         return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": str(e)}
    
#     y_pred = model.predict(X_test)
#     accuracy = round(accuracy_score(y_test, y_pred), 2)
#     exec_time = round(time.time() - start_time, 2)
    
#     response = {
#         "worker_id": f"worker{worker_id}",
#         "request_id": request_id,
#         "accuracy": accuracy,
#         "execution_time": exec_time,
#         "hyperparameters": hyperparams
#     }
#     logging.info(f"Worker {worker_id} finished task. Response: {json.dumps(response, indent=2)}")
#     return response

# def kafka_task_loop():
#     """
#     Kafka task loop: Each worker consumes messages from 'task_requests' 
#     and publishes results to 'task_responses'.
#     """
#     consumer = KafkaConsumer(
#         "task_requests",
#         bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#         value_deserializer=lambda m: json.loads(m.decode("utf-8")),
#         auto_offset_reset="earliest",
#         group_id=f"worker-group-{os.environ.get('WORKER_ID', 'X')}"
#     )
#     producer = KafkaProducer(
#         bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#         value_serializer=lambda v: json.dumps(v).encode("utf-8")
#     )
#     for message in consumer:
#         task = message.value
#         result = process_task(task)
#         producer.send("task_responses", result)
#         producer.flush()

# if __name__ == "__main__":
#     kafka_task_loop()


# import os
# import time
# import json
# import requests
# import pandas as pd
# import logging
# from sklearn.model_selection import train_test_split
# from xgboost import XGBClassifier
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.metrics import accuracy_score
# from sklearn.impute import SimpleImputer
# from kafka import KafkaConsumer, KafkaProducer
# from kafka.errors import KafkaError
# import backoff

# # Import Kaggle API for dataset downloads via Kaggle
# from kaggle.api.kaggle_api_extended import KaggleApi

# # Set up logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)

# # Kafka configuration
# KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")
# MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
# RETRY_DELAY = int(os.getenv("RETRY_DELAY", "5"))

# # Cache for downloaded datasets (maps dataset URL to local file path)
# DOWNLOADED_DATASETS = {}

# @backoff.on_exception(backoff.expo, requests.exceptions.RequestException, max_tries=3)
# def download_http_file(dataset_url, local_path):
#     """
#     Download the dataset via HTTP in chunks (1 MB per chunk) and save to local_path.
#     Uses exponential backoff for retries.
#     """
#     if os.path.exists(local_path):
#         logging.info(f"File already exists: {local_path}")
#         return local_path
#     logging.info(f"Downloading dataset from {dataset_url} via HTTP...")
#     with requests.get(dataset_url, stream=True) as r:
#         r.raise_for_status()
#         with open(local_path, 'wb') as f:
#             for chunk in r.iter_content(chunk_size=1024*1024):
#                 f.write(chunk)
#     logging.info(f"Downloaded dataset to {local_path}")
#     return local_path

# @backoff.on_exception(backoff.expo, Exception, max_tries=3)
# def download_kaggle_dataset(dataset_url, download_dir):
#     """
#     Download a Kaggle dataset using the Kaggle API with retry logic.
#     Expects a URL in the form:
#        https://www.kaggle.com/datasets/<owner>/<dataset-name>
#     Downloads and unzips dataset files into download_dir and returns the path
#     to the first CSV file found.
#     """
#     parts = dataset_url.strip().split('/')
#     if len(parts) < 6:
#         raise ValueError("Invalid Kaggle dataset URL format.")
#     dataset_id = parts[-2] + "/" + parts[-1]
#     logging.info(f"Downloading Kaggle dataset {dataset_id} into {download_dir} ...")
#     api = KaggleApi()
#     api.authenticate()
#     api.dataset_download_files(dataset_id, path=download_dir, unzip=True)
#     # Find the first CSV file in the download directory.
#     for file in os.listdir(download_dir):
#         if file.endswith(".csv"):
#             csv_path = os.path.join(download_dir, file)
#             logging.info(f"Found CSV file: {csv_path}")
#             return csv_path
#     raise ValueError("No CSV file found in the Kaggle dataset.")

# def download_file(dataset_url, local_path):
#     """
#     Decide which method to use for downloading based on the URL.
#     If the URL contains 'kaggle.com/datasets', use the Kaggle API.
#     Otherwise, use HTTP download.
#     """
#     if "kaggle.com/datasets" in dataset_url:
#         # Ensure the download directory exists
#         download_dir = os.path.dirname(local_path)
#         os.makedirs(download_dir, exist_ok=True)
#         return download_kaggle_dataset(dataset_url, download_dir)
#     else:
#         return download_http_file(dataset_url, local_path)

# def load_large_dataset(local_path, target_column=None):
#     """
#     Load a subset of the CSV dataset from local_path.
#     For testing, we sample 50,000 rows.
#     Assumes the CSV file has headers.
    
#     Args:
#         local_path: Path to the CSV file
#         target_column: Name or index of the target column. If None, assumes it's the last column.
#     """
#     compression = "gzip" if local_path.endswith(".gz") else None
#     df = pd.read_csv(local_path, compression=compression, header=0)
#     if len(df) > 50000:
#         df = df.sample(n=50000, random_state=42)
#     logging.info(f"Loaded dataset from {local_path} with {len(df)} rows.")
    
#     # Handle target column selection
#     if target_column is None:
#         # Assume the target column is the last column
#         X = df.iloc[:, :-1].values
#         y = df.iloc[:, -1].values
#     elif isinstance(target_column, str):
#         # Target column specified by name
#         X = df.drop(columns=[target_column]).values
#         y = df[target_column].values
#     else:
#         # Target column specified by index
#         X = df.drop(df.columns[target_column], axis=1).values
#         y = df.iloc[:, target_column].values
        
#     return X, y

# def process_task(task):
#     """
#     Process an ML task:
#       - Download dataset (via Kaggle API if needed) if not already cached.
#       - Load and sample the data.
#       - Impute missing values.
#       - Split data into train and test sets.
#       - Train a model (XGBoost, RandomForest, or KNN) based on provided hyperparameters.
#       - Return model accuracy and execution time.
#     """
#     worker_id = os.environ.get("WORKER_ID", "X")
#     dataset_url = task["dataset_id"]
#     algorithm = task["algorithm"]
#     hyperparams = task["hyperparameters"]
#     request_id = task["request_id"]
#     target_column = task.get("target_column", None)

#     # Determine local file path based on URL basename.
#     basename = os.path.basename(dataset_url)
#     if not (basename.endswith(".csv") or basename.endswith(".csv.gz")):
#         basename += ".csv"
#     local_path = f"/tmp/{basename}"
    
#     # Download dataset if not cached.
#     if dataset_url not in DOWNLOADED_DATASETS:
#         try:
#             local_path = download_file(dataset_url, local_path)
#             DOWNLOADED_DATASETS[dataset_url] = local_path
#         except Exception as e:
#             logger.error(f"Download error: {e}", exc_info=True)
#             return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": str(e)}
    
#     # Load the dataset
#     try:
#         X, y = load_large_dataset(DOWNLOADED_DATASETS[dataset_url], target_column)
#     except Exception as e:
#         logger.error(f"Dataset loading error: {e}", exc_info=True)
#         return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": f"Error loading dataset: {e}"}
    
#     # Impute missing values
#     imputer = SimpleImputer(strategy='mean')
#     X = imputer.fit_transform(X)
    
#     # Split data (70% train, 30% test)
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
#     start_time = time.time()
    
#     # Choose the model based on the specified algorithm
#     try:
#         if algorithm == "XGBoost":
#             model = XGBClassifier(
#                 n_estimators=int(hyperparams.get("n_estimators", "100")),
#                 max_depth=int(hyperparams.get("max_depth", "5")),
#                 learning_rate=float(hyperparams.get("learning_rate", "0.1"))
#             )
#         elif algorithm == "RandomForest":
#             model = RandomForestClassifier(
#                 n_estimators=int(hyperparams.get("n_estimators", "100")),
#                 max_depth=int(hyperparams.get("max_depth", "5"))
#             )
#         elif algorithm == "KNN":
#             model = KNeighborsClassifier(
#                 n_neighbors=int(hyperparams.get("n_neighbors", "5"))
#             )
#         else:
#             return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": f"Unsupported algorithm: {algorithm}"}
#     except Exception as e:
#         logger.error(f"Model initialization error: {e}", exc_info=True)
#         return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": f"Model initialization error: {str(e)}"}
    
#     try:
#         model.fit(X_train, y_train)
#     except Exception as e:
#         logger.error(f"Error during model.fit: {e}", exc_info=True)
#         return {"worker_id": f"worker{worker_id}", "request_id": request_id, "error": str(e)}
    
#     y_pred = model.predict(X_test)
#     accuracy = round(accuracy_score(y_test, y_pred), 4)
#     exec_time = round(time.time() - start_time, 2)
    
#     response = {
#         "worker_id": f"worker{worker_id}",
#         "request_id": request_id,
#         "accuracy": accuracy,
#         "execution_time": exec_time,
#         "hyperparameters": hyperparams,
#         "status": "success"
#     }
#     logging.info(f"Worker {worker_id} finished task. Response: {json.dumps(response, indent=2)}")
#     return response

# def create_kafka_consumer(max_retries=3):
#     """Create a Kafka consumer with retry logic"""
#     for attempt in range(max_retries):
#         try:
#             consumer = KafkaConsumer(
#                 "task_requests",
#                 bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#                 value_deserializer=lambda m: json.loads(m.decode("utf-8")),
#                 auto_offset_reset="earliest",
#                 group_id=f"worker-group-{os.environ.get('WORKER_ID', 'X')}",
#                 session_timeout_ms=30000,
#                 heartbeat_interval_ms=10000
#             )
#             return consumer
#         except KafkaError as e:
#             if attempt < max_retries - 1:
#                 logger.warning(f"Failed to connect to Kafka (attempt {attempt+1}/{max_retries}): {e}")
#                 time.sleep(5)
#             else:
#                 logger.error(f"Failed to connect to Kafka after {max_retries} attempts: {e}")
#                 raise

# def create_kafka_producer(max_retries=3):
#     """Create a Kafka producer with retry logic"""
#     for attempt in range(max_retries):
#         try:
#             producer = KafkaProducer(
#                 bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#                 value_serializer=lambda v: json.dumps(v).encode("utf-8"),
#                 retries=3
#             )
#             return producer
#         except KafkaError as e:
#             if attempt < max_retries - 1:
#                 logger.warning(f"Failed to create Kafka producer (attempt {attempt+1}/{max_retries}): {e}")
#                 time.sleep(5)
#             else:
#                 logger.error(f"Failed to create Kafka producer after {max_retries} attempts: {e}")
#                 raise

# def health_check():
#     """Return worker health status"""
#     return {
#         "status": "healthy",
#         "worker_id": os.environ.get("WORKER_ID", "X"),
#         "timestamp": time.time()
#     }

# def kafka_task_loop():
#     """
#     Kafka task loop: Each worker consumes messages from 'task_requests' 
#     and publishes results to 'task_responses'.
#     Implements reconnection logic for Kafka.
#     """
#     while True:
#         try:
#             consumer = create_kafka_consumer()
#             producer = create_kafka_producer()
            
#             logger.info(f"Worker {os.environ.get('WORKER_ID', 'X')} connected to Kafka, waiting for tasks...")
            
#             for message in consumer:
#                 task = message.value
#                 logger.info(f"Received task: {task.get('request_id')}")
                
#                 try:
#                     result = process_task(task)
#                     producer.send("task_responses", result)
#                     producer.flush()
#                     logger.info(f"Task {task.get('request_id')} completed and result sent")
#                 except Exception as e:
#                     logger.error(f"Error processing task: {e}", exc_info=True)
#                     error_result = {
#                         "worker_id": f"worker{os.environ.get('WORKER_ID', 'X')}",
#                         "request_id": task.get("request_id", "unknown"),
#                         "error": str(e),
#                         "status": "error"
#                     }
#                     producer.send("task_responses", error_result)
#                     producer.flush()
                
#         except KafkaError as e:
#             logger.error(f"Kafka connection error: {e}", exc_info=True)
#             time.sleep(5)  # Wait before reconnecting
#         except Exception as e:
#             logger.error(f"Unexpected error in Kafka task loop: {e}", exc_info=True)
#             time.sleep(5)  # Wait before reconnecting

# if __name__ == "__main__":
#     logger.info(f"Worker {os.environ.get('WORKER_ID', 'X')} starting...")
#     kafka_task_loop()


import os
import time
import logging
import uuid
import json
import pandas as pd
import numpy as np
import requests
from io import BytesIO
from zipfile import ZipFile
from kafka import KafkaConsumer, KafkaProducer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from xgboost import XGBClassifier, XGBRegressor
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_kafka_consumer():
    """Create and return a Kafka consumer for the task_requests topic."""
    try:
        consumer = KafkaConsumer(
            'task_requests',
            bootstrap_servers=['kafka:9092'],
            group_id='worker_group',
            auto_offset_reset='earliest',
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
        logger.info("Kafka consumer created successfully")
        return consumer
    except Exception as e:
        logger.error(f"Error creating Kafka consumer: {e}")
        raise

def create_kafka_producer():
    """Create and return a Kafka producer."""
    try:
        producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
        logger.info("Kafka producer created successfully")
        return producer
    except Exception as e:
        logger.error(f"Error creating Kafka producer: {e}")
        raise

def download_dataset(dataset_id):
    """
    Download a dataset from a URL or data source.
    Returns the local path to the downloaded dataset.
    """
    try:
        # Create a directory for datasets if it doesn't exist
        os.makedirs("datasets", exist_ok=True)
        
        # Generate a unique filename
        local_filename = os.path.join("datasets", f"{uuid.uuid4()}.data")
        
        # Download the dataset
        logger.info(f"Downloading dataset from {dataset_id}")
        response = requests.get(dataset_id, stream=True)
        response.raise_for_status()  # Raise an exception for HTTP errors
        
        # Check if the content is a zip file
        content_type = response.headers.get('Content-Type', '')
        compression = None
        
        if 'zip' in content_type or dataset_id.endswith('.zip'):
            # Handle zip files
            with ZipFile(BytesIO(response.content)) as zip_file:
                # Extract the first CSV file found
                csv_files = [f for f in zip_file.namelist() if f.endswith('.csv') or f.endswith('.data')]
                if csv_files:
                    with zip_file.open(csv_files[0]) as file, open(local_filename, 'wb') as out_file:
                        out_file.write(file.read())
                else:
                    raise ValueError("No CSV or data file found in the ZIP archive")
        else:
            # Handle direct file download
            with open(local_filename, 'wb') as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)
            
            # Check if it's a compressed file based on the extension
            if dataset_id.endswith('.gz'):
                compression = 'gzip'
            elif dataset_id.endswith('.zip'):
                compression = 'zip'
            elif dataset_id.endswith('.bz2'):
                compression = 'bz2'
            elif dataset_id.endswith('.xz'):
                compression = 'xz'
        
        logger.info(f"Dataset downloaded to {local_filename}")
        return local_filename
    except Exception as e:
        logger.error(f"Error downloading dataset: {e}")
        raise

def load_dataset(file_path):
    """
    Load a dataset from various formats with automatic format detection.
    """
    try:
        # Try to determine file type from extension
        file_extension = os.path.splitext(file_path)[1].lower()
        
        # Handle CSV files (try different delimiters)
        if file_extension in ['.csv', '.data', '.txt', '']:
            # Try common delimiters
            for delimiter in [',', ';', '\t', ' ']:
                try:
                    # Try with header
                    df = pd.read_csv(file_path, delimiter=delimiter, header=0)
                    # Check if parsing seems successful (more than one column)
                    if df.shape[1] > 1:
                        logging.info(f"Successfully loaded with delimiter '{delimiter}' and header")
                        return df
                except:
                    pass
                
                try:
                    # Try without header
                    df = pd.read_csv(file_path, delimiter=delimiter, header=None)
                    if df.shape[1] > 1:
                        logging.info(f"Successfully loaded with delimiter '{delimiter}' without header")
                        return df
                except:
                    pass
        
        # Handle Excel files
        elif file_extension in ['.xlsx', '.xls']:
            return pd.read_excel(file_path)
        
        # Handle JSON files
        elif file_extension == '.json':
            return pd.read_json(file_path)
        
        # Handle parquet files
        elif file_extension == '.parquet':
            return pd.read_parquet(file_path)
        
        # Handle pickle files
        elif file_extension in ['.pkl', '.pickle']:
            return pd.read_pickle(file_path)
        
        # If we get here, try a final fallback for CSV-like files
        return pd.read_csv(file_path, sep=None, engine='python')
        
    except Exception as e:
        logging.error(f"Error loading dataset: {e}")
        raise

def prepare_features_target(df, target_column=None):
    """
    Prepare features and target from dataframe with robust handling.
    """
    # Handle target column specification
    if target_column is None:
        # Assume the target column is the last column
        X = df.iloc[:, :-1]
        y = df.iloc[:, -1]
    elif isinstance(target_column, str):
        # Target column specified by name
        if target_column in df.columns:
            X = df.drop(columns=[target_column])
            y = df[target_column]
        else:
            raise ValueError(f"Target column '{target_column}' not found in dataset")
    elif isinstance(target_column, int):
        # Target column specified by index
        if 0 <= target_column < df.shape[1]:
            X = df.drop(df.columns[target_column], axis=1)
            y = df.iloc[:, target_column]
        else:
            raise ValueError(f"Target column index {target_column} out of range")
    else:
        raise ValueError(f"Invalid target column specification: {target_column}")
    
    return X, y

def detect_problem_type(y):
    """
    Automatically detect if this is a classification or regression problem.
    """
    # If target is object type or has few unique values, likely classification
    if y.dtype == object or y.dtype.name == 'category':
        return 'classification'
    
    # Check number of unique values
    unique_count = len(np.unique(y))
    if unique_count < 10 or unique_count / len(y) < 0.05:
        return 'classification'
    else:
        return 'regression'

def preprocess_data(X, y):
    """
    Apply comprehensive preprocessing to features and target.
    """
    # Handle categorical features
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns
    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns
    
    # Create preprocessing pipeline
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ]), numerical_cols),
            ('cat', Pipeline([
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', OneHotEncoder(handle_unknown='ignore'))
            ]), categorical_cols)
        ],
        remainder='passthrough'
    )
    
    # Process features
    X_processed = preprocessor.fit_transform(X)
    
    # Handle target variable
    if isinstance(y.dtype, pd.CategoricalDtype) or y.dtype == object:
        # For categorical target (classification)
        le = LabelEncoder()
        y_processed = le.fit_transform(y)
    else:
        # For numerical target (regression)
        y_processed = y
    
    return X_processed, y_processed, preprocessor

def create_model(algorithm, hyperparams, problem_type='classification'):
    """Create and return a model based on the specified algorithm and hyperparameters."""
    # Convert string parameters to appropriate types
    processed_params = {}
    for key, value in hyperparams.items():
        try:
            # Try to convert to int
            processed_params[key] = int(value)
        except ValueError:
            try:
                # Try to convert to float
                processed_params[key] = float(value)
            except ValueError:
                # Keep as string
                processed_params[key] = value
    
    if problem_type == 'classification':
        if algorithm == 'RandomForest':
            return RandomForestClassifier(**processed_params)
        elif algorithm == 'XGBoost':
            return XGBClassifier(**processed_params)
        elif algorithm == 'KNN':
            return KNeighborsClassifier(**processed_params)
        else:
            raise ValueError(f"Unsupported algorithm: {algorithm}")
    else:  # regression
        if algorithm == 'RandomForest':
            return RandomForestRegressor(**processed_params)
        elif algorithm == 'XGBoost':
            return XGBRegressor(**processed_params)
        elif algorithm == 'KNN':
            return KNeighborsRegressor(**processed_params)
        else:
            raise ValueError(f"Unsupported algorithm: {algorithm}")

def train_and_evaluate_model(X_train, X_test, y_train, y_test, algorithm, hyperparams, problem_type):
    """
    Train and evaluate a model with appropriate metrics based on problem type.
    """
    # Create and train the model
    model = create_model(algorithm, hyperparams, problem_type)
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Evaluate based on problem type
    metrics = {}
    if problem_type == 'classification':
        metrics['accuracy'] = round(float(accuracy_score(y_test, y_pred)), 4)
        try:
            metrics['f1_score'] = round(float(f1_score(y_test, y_pred, average='weighted')), 4)
        except:
            pass
    else:  # regression
        metrics['rmse'] = round(float(np.sqrt(mean_squared_error(y_test, y_pred))), 4)
        metrics['r2'] = round(float(r2_score(y_test, y_pred)), 4)
    
    return model, metrics

def process_task(task, worker_id):
    """Process a machine learning task."""
    try:
        # Extract task parameters
        request_id = task.get("request_id")
        dataset_id = task.get("dataset_id")
        algorithm = task.get("algorithm")
        hyperparams = task.get("hyperparameters", {})
        target_column = task.get("target_column")
        
        logger.info(f"Processing task with ID {request_id}, algorithm {algorithm}, hyperparams {hyperparams}")
        
        # Download dataset
        local_path = download_dataset(dataset_id)
        
        # Load dataset with enhanced loader
        df = load_dataset(local_path)
        
        # Prepare features and target
        X, y = prepare_features_target(df, target_column)
        
        # Detect problem type
        problem_type = detect_problem_type(y)
        logger.info(f"Detected problem type: {problem_type}")
        
        # Preprocess data
        X_processed, y_processed, _ = preprocess_data(X, y)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_processed, y_processed, test_size=0.2, random_state=42
        )
        
        # Train and evaluate
        start_time = time.time()
        model, metrics = train_and_evaluate_model(
            X_train, X_test, y_train, y_test, algorithm, hyperparams, problem_type
        )
        exec_time = round(time.time() - start_time, 2)
        
        # Prepare response
        response = {
            "worker_id": f"worker{worker_id}",
            "request_id": request_id,
            "execution_time": exec_time,
            "hyperparameters": hyperparams,
            "problem_type": problem_type,
            "status": "success"
        }
        
        # Add appropriate metrics based on problem type
        response.update(metrics)
        
        logger.info(f"Task completed. Metrics: {metrics}")
        
        # Clean up
        if os.path.exists(local_path):
            os.remove(local_path)
            
        return response
        
    except Exception as e:
        logger.error(f"Error processing task: {e}", exc_info=True)
        return {
            "worker_id": f"worker{worker_id}",
            "request_id": request_id if 'request_id' in locals() else "unknown",
            "status": "error",
            "error": str(e)
        }

def main():
    """Main function to run the worker node."""
    try:
        # Get worker ID from environment variable or use a default
        worker_id = os.environ.get('WORKER_ID')
        
        logger.info(f"Starting worker node with ID {worker_id}")
        
        # Create Kafka consumer and producer
        consumer = create_kafka_consumer()
        producer = create_kafka_producer()
        
        # Process messages
        for message in consumer:
            task = message.value
            logger.info(f"Received task: {task}")
            
            # Process the task
            result = process_task(task, worker_id)
            
            # Send the result back
            producer.send('task_results', result)
            producer.flush()
            logger.info(f"Sent result: {result}")
            
    except Exception as e:
        logger.error(f"Error in worker node: {e}", exc_info=True)

if __name__ == "__main__":
    main()