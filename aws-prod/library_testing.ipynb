{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3388edd9-0418-41d1-b0c8-8f273a2a3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "import redis\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, ParameterSampler\n",
    "\n",
    "class MLTaskManager:\n",
    "    def __init__(self, session_id, api_url=None, api_key=None, status_endpoint=\"/job-status\"):\n",
    "        \"\"\"\n",
    "        Initialize the task manager with a session ID and API details.\n",
    "        \n",
    "        Args:\n",
    "            session_id (str): Unique identifier for the current session\n",
    "            api_url (str): Base URL of the API endpoint\n",
    "            api_key (str): API key for authentication (if required)\n",
    "            status_endpoint (str): Endpoint for checking job status\n",
    "        \"\"\"\n",
    "        self.session_id = session_id\n",
    "        self.api_url = api_url\n",
    "        # self.api_key = api_key\n",
    "\n",
    "    \n",
    "    def extract_model_details(self, estimator):\n",
    "        \"\"\"\n",
    "        Extract model type and hyperparameters from a scikit-learn estimator.\n",
    "        \n",
    "        Args:\n",
    "            estimator: A scikit-learn estimator, GridSearchCV, or RandomizedSearchCV object\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing model type and hyperparameters\n",
    "        \"\"\"\n",
    "        if isinstance(estimator, (GridSearchCV, RandomizedSearchCV)):\n",
    "            base_estimator = estimator.estimator\n",
    "            model_type = type(base_estimator).__name__\n",
    "            search_type = \"GridSearchCV\" if isinstance(estimator, GridSearchCV) else \"RandomizedSearchCV\"\n",
    "            \n",
    "            # Get param distributions/grid\n",
    "            if isinstance(estimator, GridSearchCV):\n",
    "                param_search = {\n",
    "                    'param_grid': estimator.param_grid\n",
    "                }\n",
    "            else:  # RandomizedSearchCV\n",
    "                param_search = {\n",
    "                    'param_distributions': estimator.param_distributions,\n",
    "                    'n_iter': estimator.n_iter\n",
    "                }\n",
    "            \n",
    "            # Get CV parameters\n",
    "            cv_params = {\n",
    "                'cv': estimator.cv,\n",
    "                'scoring': estimator.scoring,\n",
    "                'refit': estimator.refit,\n",
    "                'verbose': estimator.verbose,\n",
    "                'error_score': estimator.error_score,\n",
    "                'return_train_score': estimator.return_train_score\n",
    "            }\n",
    "            \n",
    "            hyperparams = {\n",
    "                'base_estimator_params': {k.split('__')[-1]: v for k, v in base_estimator.get_params().items()},\n",
    "                'search_params': param_search,\n",
    "                'cv_params': cv_params\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'model_type': model_type,\n",
    "                'search_type': search_type,\n",
    "                'hyperparameters': hyperparams\n",
    "            }\n",
    "        else:\n",
    "            model_type = type(estimator).__name__\n",
    "            hyperparams = {k: v for k, v in estimator.get_params().items()}\n",
    "            \n",
    "            return {\n",
    "                'model_type': model_type,\n",
    "                'hyperparameters': hyperparams\n",
    "            }\n",
    "    \n",
    "    def api_request(self, endpoint, method=\"post\", data=None, params=None):\n",
    "        \"\"\"\n",
    "        Make an API request with proper error handling.\n",
    "        \n",
    "        Args:\n",
    "            endpoint (str): API endpoint (appended to base api_url)\n",
    "            method (str): HTTP method (post, get, etc.)\n",
    "            data (dict): Data to send in the request body\n",
    "            params (dict): URL parameters\n",
    "            \n",
    "        Returns:\n",
    "            dict: API response\n",
    "        \"\"\"\n",
    "        if not self.api_url:\n",
    "            print(\"Warning: API URL not provided.\")\n",
    "            return {\"status\": \"error\", \"message\": \"API URL not provided\"}\n",
    "        \n",
    "        url = f\"{self.api_url.rstrip('/')}/{endpoint.lstrip('/')}\"\n",
    "\n",
    "        \n",
    "        headers = {}\n",
    "        # if self.api_key:\n",
    "        #     headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n",
    "        \n",
    "        if data:\n",
    "            headers[\"Content-Type\"] = \"application/json\"\n",
    "        \n",
    "        try:\n",
    "            # Convert numpy/pandas types to native Python types\n",
    "            def json_serializer(obj):\n",
    "                if isinstance(obj, (np.float32, np.float64)):\n",
    "                    if math.isnan(obj) or math.isinf(obj):\n",
    "                        return None  # Convert NaN and Inf to null\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, (np.int32, np.int64)):\n",
    "                    return int(obj)\n",
    "                elif isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                elif isinstance(obj, (pd.DataFrame, pd.Series)):\n",
    "                    return obj.to_dict()\n",
    "                return str(obj)\n",
    "\n",
    "            def clean_dict(data):\n",
    "                \"\"\"Recursively replace NaN and Inf values in a dictionary with None.\"\"\"\n",
    "                if isinstance(data, dict):\n",
    "                    return {k: clean_dict(v) for k, v in data.items()}\n",
    "                elif isinstance(data, list):\n",
    "                    return [clean_dict(v) for v in data]\n",
    "                elif isinstance(data, float) and (math.isnan(data) or math.isinf(data)):\n",
    "                    return None  # Convert NaN/Inf to None\n",
    "                return data\n",
    "\n",
    "            \n",
    "            if data:\n",
    "                data = json.loads(json.dumps(data, default=json_serializer))\n",
    "                data = clean_dict(data) \n",
    "            \n",
    "            if method.lower() == \"get\":\n",
    "                response = requests.get(url, params=params, headers=headers)\n",
    "            elif method.lower() == \"post\":\n",
    "                response = requests.post(url, json=data, params=params, headers=headers)\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"message\": f\"Unsupported HTTP method: {method}\"}\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API request failed: {str(e)}\")\n",
    "            print(f\"ERROR HERE\")\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing request: {str(e)}\")\n",
    "            print(f\"ERROR HERE\")\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "    def send_to_training_api(self, payload):\n",
    "        \"\"\"\n",
    "        Send job configuration to the distributed training API endpoint.\n",
    "        \n",
    "        Args:\n",
    "            payload (dict): The data to send to the API\n",
    "            \n",
    "        Returns:\n",
    "            dict: API response\n",
    "        \"\"\"\n",
    "        return self.api_request(f\"train/{self.session_id}\", method=\"post\", data=payload)\n",
    "    \n",
    "    def check_job_status(self, job_id):\n",
    "        \"\"\"\n",
    "        Check the status of a training job.\n",
    "        \n",
    "        Args:\n",
    "            job_id (str): ID of the job to check\n",
    "            \n",
    "        Returns:\n",
    "            dict: Job status information\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.api_request(\n",
    "            f\"check_status/{self.session_id}/{job_id}\",\n",
    "            method=\"get\", \n",
    "        )\n",
    "    \n",
    "    # def wait_for_completion(self, job_id, polling_interval=5, timeout=3600):\n",
    "    #     \"\"\"\n",
    "    #     Wait for a job to complete, with a progress bar.\n",
    "        \n",
    "    #     Args:\n",
    "    #         job_id (str): ID of the job to monitor\n",
    "    #         polling_interval (int): Seconds between status checks\n",
    "    #         timeout (int): Maximum seconds to wait\n",
    "            \n",
    "    #     Returns:\n",
    "    #         dict: Final job status\n",
    "    #     \"\"\"\n",
    "    #     start_time = time.time()\n",
    "    #     progress_bar = None\n",
    "    #     last_progress = 0\n",
    "    #     total_tasks = 0\n",
    "        \n",
    "    #     print(f\"Monitoring job: {job_id}\")\n",
    "        \n",
    "    #     while True:\n",
    "    #         # Check if we've exceeded the timeout\n",
    "    #         if time.time() - start_time > timeout:\n",
    "    #             if progress_bar:\n",
    "    #                 progress_bar.close()\n",
    "    #             return {\"status\": \"error\", \"message\": f\"Timeout after {timeout} seconds\"}\n",
    "            \n",
    "    #         # Get job status\n",
    "    #         status = self.check_job_status(job_id)\n",
    "            \n",
    "    #         # Handle error in status check\n",
    "    #         if status.get(\"job_status\") == \"error\":\n",
    "    #             if progress_bar:\n",
    "    #                 progress_bar.close()\n",
    "    #             return status\n",
    "            \n",
    "    #         # Extract progress information\n",
    "    #         job_status = status.get(\"job_status\", \"unknown\")\n",
    "    #         current_progress = status.get(\"progress\", {})\n",
    "    #         completed_tasks = current_progress.get(\"completed_tasks\", 0)\n",
    "            \n",
    "    #         # If this is the first time we're getting task information, set up the progress bar\n",
    "    #         if total_tasks == 0 and \"total_subtasks\" in current_progress:\n",
    "    #             total_tasks = current_progress.get(\"total_subtasks\", 100)\n",
    "    #             progress_bar = tqdm(total=total_tasks, desc=\"Training Progress\")\n",
    "            \n",
    "    #         # Update progress bar if it exists\n",
    "    #         if progress_bar and completed_tasks > last_progress:\n",
    "    #             progress_bar.update(completed_tasks - last_progress)\n",
    "    #             last_progress = completed_tasks\n",
    "            \n",
    "    #         # Check if job is complete\n",
    "    #         if job_status.lower() in [\"completed\", \"failed\", \"error\"]:\n",
    "    #             if progress_bar:\n",
    "    #                 # Ensure progress bar is at 100% for completed jobs\n",
    "    #                 if job_status.lower() == \"completed\" and last_progress < total_tasks:\n",
    "    #                     progress_bar.update(total_tasks - last_progress)\n",
    "    #                 progress_bar.close()\n",
    "                \n",
    "    #             # Return the final status\n",
    "    #             return status\n",
    "            \n",
    "    #         # Wait before polling again\n",
    "    #         time.sleep(polling_interval)\n",
    "\n",
    "    def wait_for_completion(self, job_id, polling_interval=5, timeout=3600):\n",
    "        \"\"\"\n",
    "        Wait for a job to complete, with a progress bar and structured response for frontend.\n",
    "        \n",
    "        Args:\n",
    "            job_id (str): ID of the job to monitor\n",
    "            polling_interval (int): Seconds between status checks\n",
    "            timeout (int): Maximum seconds to wait\n",
    "            \n",
    "        Returns:\n",
    "            dict: Final job status in structured format\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        progress_bar = None\n",
    "        last_progress = 0\n",
    "        total_tasks = 0\n",
    "        \n",
    "        print(f\"Monitoring job: {job_id}\")\n",
    "        \n",
    "        while True:\n",
    "            # Check if we've exceeded the timeout\n",
    "            if time.time() - start_time > timeout:\n",
    "                if progress_bar:\n",
    "                    progress_bar.close()\n",
    "                return {\n",
    "                    \"job_id\": job_id,\n",
    "                    \"status\": \"error\",\n",
    "                    \"completed_tasks\": last_progress,\n",
    "                    \"total_subtasks\": total_tasks,\n",
    "                    \"progress_percentage\": (last_progress / total_tasks) * 100 if total_tasks else 0,\n",
    "                    \"message\": f\"Timeout after {timeout} seconds\"\n",
    "                }\n",
    "            \n",
    "            # Get job status\n",
    "            status = self.check_job_status(job_id)\n",
    "            print(json.dumps(status, indent=2))\n",
    "            \n",
    "            # Handle error in status check\n",
    "            if status.get(\"job_status\") == \"error\":\n",
    "                if progress_bar:\n",
    "                    progress_bar.close()\n",
    "                return {\n",
    "                    \"job_id\": job_id,\n",
    "                    \"status\": \"error\",\n",
    "                    \"completed_tasks\": last_progress,\n",
    "                    \"total_subtasks\": total_tasks,\n",
    "                    \"progress_percentage\": (last_progress / total_tasks) * 100 if total_tasks else 0,\n",
    "                    \"message\": status.get(\"message\", \"An error occurred while fetching job status\")\n",
    "                }\n",
    "            \n",
    "            # Extract progress information\n",
    "            job_status = status.get(\"job_status\", \"unknown\")\n",
    "            completed_tasks = status.get(\"completed_tasks\", 0)\n",
    "            total_tasks = status.get(\"total_subtasks\", 100)  # Default to 100 if unknown\n",
    "            \n",
    "            # Initialize progress bar\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=total_tasks, desc=\"Training Progress\")\n",
    "            \n",
    "            # Update progress bar if progress has changed\n",
    "            if progress_bar and completed_tasks > last_progress:\n",
    "                progress_bar.update(completed_tasks - last_progress)\n",
    "                last_progress = completed_tasks\n",
    "            \n",
    "            # Calculate progress percentage\n",
    "            progress_percentage = (completed_tasks / total_tasks) * 100 if total_tasks else 0\n",
    "    \n",
    "            # If job is completed, return response\n",
    "            if job_status.lower() in [\"completed\", \"failed\", \"error\"]:\n",
    "                if progress_bar:\n",
    "                    if job_status.lower() == \"completed\" and last_progress < total_tasks:\n",
    "                        progress_bar.update(total_tasks - last_progress)\n",
    "                    progress_bar.close()\n",
    "                \n",
    "                return {\n",
    "                    \"job_id\": job_id,\n",
    "                    \"status\": job_status,\n",
    "                    \"completed_tasks\": completed_tasks,\n",
    "                    \"total_subtasks\": total_tasks,\n",
    "                    \"progress_percentage\": progress_percentage,\n",
    "                    \"message\": \"Job completed successfully\" if job_status == \"completed\" else f\"Job ended with status: {job_status}\"\n",
    "                }\n",
    "            \n",
    "            # Wait before polling again\n",
    "            time.sleep(polling_interval)\n",
    "    \n",
    "            # Return intermediate status for frontend polling\n",
    "            return {\n",
    "                \"job_id\": job_id,\n",
    "                \"status\": job_status,\n",
    "                \"completed_tasks\": completed_tasks,\n",
    "                \"total_subtasks\": total_tasks,\n",
    "                \"progress_percentage\": progress_percentage,\n",
    "                \"message\": \"Job is still in progress\"\n",
    "            }\n",
    "    \n",
    "    def create_job(self, estimator, dataset_id, train_params=None, wait_for_completion=False, polling_interval=5, timeout=3600):\n",
    "        \"\"\"\n",
    "        Create a new job configuration without sending dataset data.\n",
    "        \n",
    "        Args:\n",
    "            estimator: A scikit-learn estimator, GridSearchCV, or RandomizedSearchCV object\n",
    "            dataset_id (str): ID of the dataset to be used for training (already on backend)\n",
    "            train_params (dict): Additional training parameters (test_size, random_state, etc.)\n",
    "            wait_for_completion (bool): Whether to wait for the job to complete\n",
    "            polling_interval (int): Seconds between status checks if waiting\n",
    "            timeout (int): Maximum seconds to wait if waiting\n",
    "            \n",
    "        Returns:\n",
    "            dict: Response containing status and job details\n",
    "        \"\"\"\n",
    "        # Generate a unique job ID\n",
    "        job_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Extract model details\n",
    "        model_details = self.extract_model_details(estimator)\n",
    "        \n",
    "        # Set default train parameters if not provided\n",
    "        if train_params is None:\n",
    "            train_params = {}\n",
    "        \n",
    "        # Add default test_size if not specified\n",
    "        if 'test_size' not in train_params and not isinstance(estimator, (GridSearchCV, RandomizedSearchCV)):\n",
    "            train_params['test_size'] = 0.2\n",
    "        \n",
    "        # Create the job configuration\n",
    "        job_config = {\n",
    "            'job_id': job_id,\n",
    "            'session_id': self.session_id,\n",
    "            'dataset_id': dataset_id,\n",
    "            'model_details': model_details,\n",
    "            'train_params': train_params,\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Send to training API\n",
    "        api_response = self.send_to_training_api(job_config)\n",
    "        \n",
    "        # Print results for debugging\n",
    "        print(f\"Created job configuration: {json.dumps(job_config, indent=2)}\")\n",
    "        print(f\"API response: {api_response}\")\n",
    "        \n",
    "        result = {\n",
    "            'status': 'success' if api_response.get(\"status\") != \"error\" else 'error', \n",
    "            'job_id': job_id,\n",
    "            'job_config': job_config,\n",
    "            'api_response': api_response\n",
    "        }\n",
    "        \n",
    "        # Wait for job completion if requested\n",
    "        if wait_for_completion and api_response.get(\"status\") != \"error\":\n",
    "            print(f\"Waiting for job {job_id} to complete...\")\n",
    "            final_status = self.wait_for_completion(job_id, polling_interval, timeout)\n",
    "            result['final_status'] = final_status\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "94b8aaf2-d4bd-4f8f-af0f-882b71903456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_subtasks(job_config):\n",
    "#     \"\"\"\n",
    "#     Create subtasks for distributed processing based on the job configuration.\n",
    "    \n",
    "#     Args:\n",
    "#         job_config (dict): The job configuration from create_job()\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of subtask configurations\n",
    "#     \"\"\"\n",
    "#     job_id = job_config['job_id']\n",
    "#     session_id = job_config['session_id']\n",
    "#     dataset_id = job_config['dataset_id']\n",
    "#     model_details = job_config['model_details']\n",
    "#     train_params = job_config.get('train_params', {})\n",
    "    \n",
    "#     subtasks = []\n",
    "    \n",
    "#     # If this is a search job (GridSearchCV or RandomizedSearchCV)\n",
    "#     if 'search_type' in model_details:\n",
    "#         search_type = model_details['search_type']\n",
    "#         base_model_type = model_details['model_type']\n",
    "#         base_params = model_details['hyperparameters']['base_estimator_params']\n",
    "#         cv_params = model_details['hyperparameters'].get('cv_params', {})\n",
    "#         search_params = model_details['hyperparameters'].get('search_params', {})\n",
    "        \n",
    "#         # Get CV value for train/test splitting\n",
    "#         cv = cv_params.get('cv', 5)\n",
    "        \n",
    "#         # For GridSearchCV, create one subtask per parameter combination\n",
    "#         if search_type == 'GridSearchCV':\n",
    "#             param_grid = search_params.get('param_grid', {})\n",
    "#             param_combinations = list(ParameterGrid(param_grid))\n",
    "            \n",
    "#             for i, params in enumerate(param_combinations):\n",
    "#                 # Combine base parameters with this specific combination\n",
    "#                 full_params = {**base_params, **params}\n",
    "                \n",
    "#                 subtask_id = f\"{job_id}-subtask-{i+1}\"\n",
    "#                 subtask = {\n",
    "#                     'subtask_id': subtask_id,\n",
    "#                     'job_id': job_id,\n",
    "#                     'session_id': session_id,\n",
    "#                     'dataset_id': dataset_id,\n",
    "#                     'model_type': base_model_type,\n",
    "#                     'parameters': full_params,\n",
    "#                     'train_params': {\n",
    "#                         'cv': cv,  # Use CV value for validation\n",
    "#                         **train_params\n",
    "#                     }\n",
    "#                 }\n",
    "#                 subtasks.append(subtask)\n",
    "        \n",
    "#         # For RandomizedSearchCV, create requested number of random combinations\n",
    "#         elif search_type == 'RandomizedSearchCV':\n",
    "#             param_distributions = search_params.get('param_distributions', {})\n",
    "#             n_iter = search_params.get('n_iter', 10)\n",
    "#             random_state = cv_params.get('random_state', 42)\n",
    "            \n",
    "#             # Generate random parameter combinations\n",
    "#             param_combinations = list(ParameterSampler(\n",
    "#                 param_distributions, n_iter, random_state=random_state))\n",
    "            \n",
    "#             for i, params in enumerate(param_combinations):\n",
    "#                 # Combine base parameters with this specific combination\n",
    "#                 full_params = {**base_params, **params}\n",
    "                \n",
    "#                 subtask_id = f\"{job_id}-subtask-{i+1}\"\n",
    "#                 subtask = {\n",
    "#                     'subtask_id': subtask_id,\n",
    "#                     'job_id': job_id,\n",
    "#                     'session_id': session_id,\n",
    "#                     'dataset_id': dataset_id,\n",
    "#                     'model_type': base_model_type,\n",
    "#                     'parameters': full_params,\n",
    "#                     'train_params': {\n",
    "#                         'cv': cv,  # Use CV value for validation\n",
    "#                         **train_params\n",
    "#                     }\n",
    "#                 }\n",
    "#                 subtasks.append(subtask)\n",
    "    \n",
    "#     # For regular estimator, create a single subtask\n",
    "#     else:\n",
    "#         subtask_id = f\"{job_id}-subtask-1\"\n",
    "#         subtask = {\n",
    "#             'subtask_id': subtask_id,\n",
    "#             'job_id': job_id,\n",
    "#             'session_id': session_id,\n",
    "#             'dataset_id': dataset_id,\n",
    "#             'model_type': model_details['model_type'],\n",
    "#             'parameters': model_details['hyperparameters'],\n",
    "#             'train_params': train_params\n",
    "#         }\n",
    "#         subtasks.append(subtask)\n",
    "    \n",
    "#     return subtasks\n",
    "\n",
    "\n",
    "# def save_subtasks_to_redis(subtasks, redis_config=None):\n",
    "#     \"\"\"\n",
    "#     Save subtasks to Redis under session_id -> job_id -> subtasks structure.\n",
    "    \n",
    "#     Args:\n",
    "#         subtasks (list): List of subtask configurations\n",
    "#         redis_config (dict): Redis connection configuration\n",
    "        \n",
    "#     Returns:\n",
    "#         dict: Result of the operation\n",
    "#     \"\"\"\n",
    "#     if not subtasks:\n",
    "#         return {\"status\": \"error\", \"message\": \"No subtasks to save\"}\n",
    "    \n",
    "#     # Default Redis configuration\n",
    "#     if redis_config is None:\n",
    "#         redis_config = {\n",
    "#             'host': 'localhost',\n",
    "#             'port': 6379,\n",
    "#             'db': 0,\n",
    "#             'password': None\n",
    "#         }\n",
    "    \n",
    "#     try:\n",
    "#         # Connect to Redis\n",
    "#         r = redis.Redis(\n",
    "#             host=redis_config['host'],\n",
    "#             port=redis_config['port'],\n",
    "#             db=redis_config['db'],\n",
    "#             password=redis_config['password'],\n",
    "#             decode_responses=True\n",
    "#         )\n",
    "        \n",
    "#         # Get session_id and job_id from the first subtask\n",
    "#         session_id = subtasks[0]['session_id']\n",
    "#         job_id = subtasks[0]['job_id']\n",
    "        \n",
    "#         # Create key structure\n",
    "#         session_key = f\"ml:sessions:{session_id}\"\n",
    "#         job_key = f\"{session_key}:jobs:{job_id}\"\n",
    "        \n",
    "#         # Store job metadata\n",
    "#         r.hset(job_key, \"total_subtasks\", len(subtasks))\n",
    "#         r.hset(job_key, \"completed_subtasks\", 0)\n",
    "#         r.hset(job_key, \"status\", \"pending\")\n",
    "#         r.hset(job_key, \"created_at\", pd.Timestamp.now().isoformat())\n",
    "        \n",
    "#         # Add job to session list\n",
    "#         r.sadd(f\"{session_key}:jobs\", job_id)\n",
    "        \n",
    "#         # Save each subtask\n",
    "#         for subtask in subtasks:\n",
    "#             subtask_id = subtask['subtask_id']\n",
    "#             subtask_key = f\"{job_key}:subtasks:{subtask_id}\"\n",
    "            \n",
    "#             # Convert subtask to JSON\n",
    "#             subtask_json = json.dumps(subtask, default=str)\n",
    "            \n",
    "#             # Store subtask\n",
    "#             r.set(subtask_key, subtask_json)\n",
    "            \n",
    "#             # Add to pending subtasks list\n",
    "#             r.rpush(f\"{job_key}:pending_subtasks\", subtask_id)\n",
    "            \n",
    "#             # Add to subtasks set\n",
    "#             r.sadd(f\"{job_key}:subtasks\", subtask_id)\n",
    "        \n",
    "#         return {\n",
    "#             \"status\": \"success\",\n",
    "#             \"message\": f\"Saved {len(subtasks)} subtasks to Redis\",\n",
    "#             \"session_id\": session_id,\n",
    "#             \"job_id\": job_id\n",
    "#         }\n",
    "    \n",
    "#     except redis.RedisError as e:\n",
    "#         return {\"status\": \"error\", \"message\": f\"Redis error: {str(e)}\"}\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         return {\"status\": \"error\", \"message\": f\"Error saving to Redis: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "da1784ea-de81-4cf9-a095-4b309ed87ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: 9db8c432-b7d6-406a-97ca-3d5315910d2e\n",
      "Created job configuration: {\n",
      "  \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "  \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "  \"dataset_id\": \"iris\",\n",
      "  \"model_details\": {\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"search_type\": \"GridSearchCV\",\n",
      "    \"hyperparameters\": {\n",
      "      \"base_estimator_params\": {\n",
      "        \"C\": 1.0,\n",
      "        \"class_weight\": null,\n",
      "        \"dual\": false,\n",
      "        \"fit_intercept\": true,\n",
      "        \"intercept_scaling\": 1,\n",
      "        \"l1_ratio\": null,\n",
      "        \"max_iter\": 100,\n",
      "        \"multi_class\": \"auto\",\n",
      "        \"n_jobs\": null,\n",
      "        \"penalty\": \"l2\",\n",
      "        \"random_state\": null,\n",
      "        \"solver\": \"lbfgs\",\n",
      "        \"tol\": 0.0001,\n",
      "        \"verbose\": 0,\n",
      "        \"warm_start\": false\n",
      "      },\n",
      "      \"search_params\": {\n",
      "        \"param_grid\": {\n",
      "          \"C\": [\n",
      "            0.1,\n",
      "            1.0,\n",
      "            10.0,\n",
      "            100\n",
      "          ],\n",
      "          \"solver\": [\n",
      "            \"liblinear\",\n",
      "            \"lbfgs\"\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      \"cv_params\": {\n",
      "        \"cv\": 5,\n",
      "        \"scoring\": null,\n",
      "        \"refit\": true,\n",
      "        \"verbose\": 0,\n",
      "        \"error_score\": NaN,\n",
      "        \"return_train_score\": false\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"train_params\": {\n",
      "    \"random_state\": 42,\n",
      "    \"feature_columns\": [\n",
      "      \"sepal_length\",\n",
      "      \"sepal_width\",\n",
      "      \"petal_length\",\n",
      "      \"petal_width\"\n",
      "    ],\n",
      "    \"target_column\": \"species\"\n",
      "  },\n",
      "  \"timestamp\": \"2025-03-15T14:52:46.668372\"\n",
      "}\n",
      "API response: {'All good': 'final test'}\n",
      "Created 8 subtasks for GridSearchCV\n",
      "[\n",
      "  {\n",
      "    \"subtask_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49-subtask-1\",\n",
      "    \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "    \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "    \"dataset_id\": \"iris\",\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"parameters\": {\n",
      "      \"C\": 0.1,\n",
      "      \"class_weight\": null,\n",
      "      \"dual\": false,\n",
      "      \"fit_intercept\": true,\n",
      "      \"intercept_scaling\": 1,\n",
      "      \"l1_ratio\": null,\n",
      "      \"max_iter\": 100,\n",
      "      \"multi_class\": \"auto\",\n",
      "      \"n_jobs\": null,\n",
      "      \"penalty\": \"l2\",\n",
      "      \"random_state\": null,\n",
      "      \"solver\": \"liblinear\",\n",
      "      \"tol\": 0.0001,\n",
      "      \"verbose\": 0,\n",
      "      \"warm_start\": false\n",
      "    },\n",
      "    \"train_params\": {\n",
      "      \"cv\": 5,\n",
      "      \"random_state\": 42,\n",
      "      \"feature_columns\": [\n",
      "        \"sepal_length\",\n",
      "        \"sepal_width\",\n",
      "        \"petal_length\",\n",
      "        \"petal_width\"\n",
      "      ],\n",
      "      \"target_column\": \"species\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"subtask_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49-subtask-2\",\n",
      "    \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "    \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "    \"dataset_id\": \"iris\",\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"parameters\": {\n",
      "      \"C\": 0.1,\n",
      "      \"class_weight\": null,\n",
      "      \"dual\": false,\n",
      "      \"fit_intercept\": true,\n",
      "      \"intercept_scaling\": 1,\n",
      "      \"l1_ratio\": null,\n",
      "      \"max_iter\": 100,\n",
      "      \"multi_class\": \"auto\",\n",
      "      \"n_jobs\": null,\n",
      "      \"penalty\": \"l2\",\n",
      "      \"random_state\": null,\n",
      "      \"solver\": \"lbfgs\",\n",
      "      \"tol\": 0.0001,\n",
      "      \"verbose\": 0,\n",
      "      \"warm_start\": false\n",
      "    },\n",
      "    \"train_params\": {\n",
      "      \"cv\": 5,\n",
      "      \"random_state\": 42,\n",
      "      \"feature_columns\": [\n",
      "        \"sepal_length\",\n",
      "        \"sepal_width\",\n",
      "        \"petal_length\",\n",
      "        \"petal_width\"\n",
      "      ],\n",
      "      \"target_column\": \"species\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"subtask_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49-subtask-3\",\n",
      "    \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "    \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "    \"dataset_id\": \"iris\",\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"parameters\": {\n",
      "      \"C\": 1.0,\n",
      "      \"class_weight\": null,\n",
      "      \"dual\": false,\n",
      "      \"fit_intercept\": true,\n",
      "      \"intercept_scaling\": 1,\n",
      "      \"l1_ratio\": null,\n",
      "      \"max_iter\": 100,\n",
      "      \"multi_class\": \"auto\",\n",
      "      \"n_jobs\": null,\n",
      "      \"penalty\": \"l2\",\n",
      "      \"random_state\": null,\n",
      "      \"solver\": \"liblinear\",\n",
      "      \"tol\": 0.0001,\n",
      "      \"verbose\": 0,\n",
      "      \"warm_start\": false\n",
      "    },\n",
      "    \"train_params\": {\n",
      "      \"cv\": 5,\n",
      "      \"random_state\": 42,\n",
      "      \"feature_columns\": [\n",
      "        \"sepal_length\",\n",
      "        \"sepal_width\",\n",
      "        \"petal_length\",\n",
      "        \"petal_width\"\n",
      "      ],\n",
      "      \"target_column\": \"species\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"subtask_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49-subtask-4\",\n",
      "    \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "    \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "    \"dataset_id\": \"iris\",\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"parameters\": {\n",
      "      \"C\": 1.0,\n",
      "      \"class_weight\": null,\n",
      "      \"dual\": false,\n",
      "      \"fit_intercept\": true,\n",
      "      \"intercept_scaling\": 1,\n",
      "      \"l1_ratio\": null,\n",
      "      \"max_iter\": 100,\n",
      "      \"multi_class\": \"auto\",\n",
      "      \"n_jobs\": null,\n",
      "      \"penalty\": \"l2\",\n",
      "      \"random_state\": null,\n",
      "      \"solver\": \"lbfgs\",\n",
      "      \"tol\": 0.0001,\n",
      "      \"verbose\": 0,\n",
      "      \"warm_start\": false\n",
      "    },\n",
      "    \"train_params\": {\n",
      "      \"cv\": 5,\n",
      "      \"random_state\": 42,\n",
      "      \"feature_columns\": [\n",
      "        \"sepal_length\",\n",
      "        \"sepal_width\",\n",
      "        \"petal_length\",\n",
      "        \"petal_width\"\n",
      "      ],\n",
      "      \"target_column\": \"species\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"subtask_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49-subtask-5\",\n",
      "    \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "    \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "    \"dataset_id\": \"iris\",\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"parameters\": {\n",
      "      \"C\": 10.0,\n",
      "      \"class_weight\": null,\n",
      "      \"dual\": false,\n",
      "      \"fit_intercept\": true,\n",
      "      \"intercept_scaling\": 1,\n",
      "      \"l1_ratio\": null,\n",
      "      \"max_iter\": 100,\n",
      "      \"multi_class\": \"auto\",\n",
      "      \"n_jobs\": null,\n",
      "      \"penalty\": \"l2\",\n",
      "      \"random_state\": null,\n",
      "      \"solver\": \"liblinear\",\n",
      "      \"tol\": 0.0001,\n",
      "      \"verbose\": 0,\n",
      "      \"warm_start\": false\n",
      "    },\n",
      "    \"train_params\": {\n",
      "      \"cv\": 5,\n",
      "      \"random_state\": 42,\n",
      "      \"feature_columns\": [\n",
      "        \"sepal_length\",\n",
      "        \"sepal_width\",\n",
      "        \"petal_length\",\n",
      "        \"petal_width\"\n",
      "      ],\n",
      "      \"target_column\": \"species\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"subtask_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49-subtask-6\",\n",
      "    \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "    \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "    \"dataset_id\": \"iris\",\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"parameters\": {\n",
      "      \"C\": 10.0,\n",
      "      \"class_weight\": null,\n",
      "      \"dual\": false,\n",
      "      \"fit_intercept\": true,\n",
      "      \"intercept_scaling\": 1,\n",
      "      \"l1_ratio\": null,\n",
      "      \"max_iter\": 100,\n",
      "      \"multi_class\": \"auto\",\n",
      "      \"n_jobs\": null,\n",
      "      \"penalty\": \"l2\",\n",
      "      \"random_state\": null,\n",
      "      \"solver\": \"lbfgs\",\n",
      "      \"tol\": 0.0001,\n",
      "      \"verbose\": 0,\n",
      "      \"warm_start\": false\n",
      "    },\n",
      "    \"train_params\": {\n",
      "      \"cv\": 5,\n",
      "      \"random_state\": 42,\n",
      "      \"feature_columns\": [\n",
      "        \"sepal_length\",\n",
      "        \"sepal_width\",\n",
      "        \"petal_length\",\n",
      "        \"petal_width\"\n",
      "      ],\n",
      "      \"target_column\": \"species\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"subtask_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49-subtask-7\",\n",
      "    \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "    \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "    \"dataset_id\": \"iris\",\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"parameters\": {\n",
      "      \"C\": 100,\n",
      "      \"class_weight\": null,\n",
      "      \"dual\": false,\n",
      "      \"fit_intercept\": true,\n",
      "      \"intercept_scaling\": 1,\n",
      "      \"l1_ratio\": null,\n",
      "      \"max_iter\": 100,\n",
      "      \"multi_class\": \"auto\",\n",
      "      \"n_jobs\": null,\n",
      "      \"penalty\": \"l2\",\n",
      "      \"random_state\": null,\n",
      "      \"solver\": \"liblinear\",\n",
      "      \"tol\": 0.0001,\n",
      "      \"verbose\": 0,\n",
      "      \"warm_start\": false\n",
      "    },\n",
      "    \"train_params\": {\n",
      "      \"cv\": 5,\n",
      "      \"random_state\": 42,\n",
      "      \"feature_columns\": [\n",
      "        \"sepal_length\",\n",
      "        \"sepal_width\",\n",
      "        \"petal_length\",\n",
      "        \"petal_width\"\n",
      "      ],\n",
      "      \"target_column\": \"species\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"subtask_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49-subtask-8\",\n",
      "    \"job_id\": \"fcc21e56-a466-4d8c-b7da-32179e4e0f49\",\n",
      "    \"session_id\": \"9db8c432-b7d6-406a-97ca-3d5315910d2e\",\n",
      "    \"dataset_id\": \"iris\",\n",
      "    \"model_type\": \"LogisticRegression\",\n",
      "    \"parameters\": {\n",
      "      \"C\": 100,\n",
      "      \"class_weight\": null,\n",
      "      \"dual\": false,\n",
      "      \"fit_intercept\": true,\n",
      "      \"intercept_scaling\": 1,\n",
      "      \"l1_ratio\": null,\n",
      "      \"max_iter\": 100,\n",
      "      \"multi_class\": \"auto\",\n",
      "      \"n_jobs\": null,\n",
      "      \"penalty\": \"l2\",\n",
      "      \"random_state\": null,\n",
      "      \"solver\": \"lbfgs\",\n",
      "      \"tol\": 0.0001,\n",
      "      \"verbose\": 0,\n",
      "      \"warm_start\": false\n",
      "    },\n",
      "    \"train_params\": {\n",
      "      \"cv\": 5,\n",
      "      \"random_state\": 42,\n",
      "      \"feature_columns\": [\n",
      "        \"sepal_length\",\n",
      "        \"sepal_width\",\n",
      "        \"petal_length\",\n",
      "        \"petal_width\"\n",
      "      ],\n",
      "      \"target_column\": \"species\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def example_usage():\n",
    "    import requests\n",
    "\n",
    "    session_id = \"\"\n",
    "    dataset_id = \"iris\"\n",
    "    columns = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "    target = 'species'\n",
    "    url = \"http://127.0.0.1:5001/create_session\"  # Replace with the actual API URL\n",
    "\n",
    "# Make a GET request to obtain the session ID\n",
    "    response = requests.post(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "    if response.status_code == 200 or 201:\n",
    "        session_id = response.json().get(\"session_id\")  # Adjust key if needed\n",
    "        print(f\"Session ID: {session_id}\")\n",
    "    else:\n",
    "        print(f\"Failed to get session ID: {response.status_code}, {response.text}\")\n",
    "        \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from scipy.stats import randint, uniform\n",
    "    \n",
    "    \n",
    "    # Initialize the task manager with API details\n",
    "    task_manager = MLTaskManager(\n",
    "        api_url=f\"http://127.0.0.1:5001\",\n",
    "        session_id=session_id,\n",
    "        api_key=\"your-api-key-here\",\n",
    "    )\n",
    "    \n",
    "    # Example 1: Create a single model job\n",
    "    # rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "    # job_response = task_manager.create_job(\n",
    "    #     rf, \n",
    "    #     dataset_id=\"iris\",\n",
    "    #     train_params={\n",
    "    #         'test_size': 0.25,\n",
    "    #         'random_state': 42,\n",
    "    #         'feature_columns': columns,\n",
    "    #         'target_column': target\n",
    "    #     }\n",
    "    # )\n",
    "    \n",
    "    # # Create subtasks for this job\n",
    "    # subtasks = create_subtasks(job_response['job_config'])\n",
    "    # print(f\"Created {len(subtasks)} subtasks\")\n",
    "    # print(json.dumps(subtasks, indent=2))\n",
    "    \n",
    "    # Save subtasks to Redis\n",
    "    # redis_result = save_subtasks_to_redis(\n",
    "    #     subtasks,\n",
    "    #     redis_config={\n",
    "    #         'host': 'redis-server.example.com',\n",
    "    #         'port': 6379,\n",
    "    #         'db': 0,\n",
    "    #         'password': 'your-redis-password'\n",
    "    #     }\n",
    "    # )\n",
    "    # print(f\"Redis result: {redis_result}\")\n",
    "    \n",
    "    # Example 2: Create a GridSearchCV job\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1.0, 10.0, 100],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "    lr = LogisticRegression()\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
    "    \n",
    "    job_response = task_manager.create_job(\n",
    "        grid_search, \n",
    "        dataset_id=dataset_id,\n",
    "        train_params={\n",
    "            'random_state': 42,\n",
    "            'feature_columns': columns,\n",
    "            'target_column': target\n",
    "        },\n",
    "        # wait_for_completion=True\n",
    "    )\n",
    "    \n",
    "    # Create and save subtasks for GridSearchCV\n",
    "    subtasks = create_subtasks(job_response['job_config'])\n",
    "    print(f\"Created {len(subtasks)} subtasks for GridSearchCV\")\n",
    "    print(json.dumps(subtasks, indent=2))\n",
    "    \n",
    "    # redis_result = save_subtasks_to_redis(subtasks)\n",
    "    # print(f\"Redis result: {redis_result}\")\n",
    "\n",
    "    # lr = LogisticRegression()\n",
    "    # random_search = RandomizedSearchCV(lr, param_grid, cv=5)\n",
    "    \n",
    "    # job_response = task_manager.create_job(\n",
    "    #     random_search, \n",
    "    #     dataset_id=dataset_id,\n",
    "    #     train_params={\n",
    "    #         'random_state': 42,\n",
    "    #         'feature_columns': columns,\n",
    "    #         'target_column': target\n",
    "    #     }\n",
    "    # )\n",
    "    \n",
    "    # # Create and save subtasks for GridSearchCV\n",
    "    # subtasks = create_subtasks(job_response['job_config'])\n",
    "    # print(f\"Created {len(subtasks)} subtasks for RandomizedSearch\")\n",
    "    # print(json.dumps(subtasks, indent=2))\n",
    "    \n",
    "    # redis_result = save_subtasks_to_redis(subtasks)\n",
    "    # print(f\"Redis result: {redis_result}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b9a6b-f5c5-42fa-a129-e1eb47b72e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f19e9-fe2f-4f50-bec8-0c1f099ee9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "328fd8d9-f15f-450d-9f59-df8e5b8733e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting confluent_kafka\n",
      "  Downloading confluent_kafka-2.8.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Downloading confluent_kafka-2.8.2-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: confluent_kafka\n",
      "Successfully installed confluent_kafka-2.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab4c2668-7026-47dc-b077-d205c565c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka Connection Successful! Available Topics: dict_keys(['result', 'results', 'train', '__consumer_offsets'])\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "conf = {'bootstrap.servers': '127.0.0.1:9092'}\n",
    "admin_client = AdminClient(conf)\n",
    "\n",
    "try:\n",
    "    topics = admin_client.list_topics(timeout=5)\n",
    "    print(\"Kafka Connection Successful! Available Topics:\", topics.topics.keys())\n",
    "except Exception as e:\n",
    "    print(\"Kafka Connection Failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "079eca7c-c54f-45db-ac4e-9e9c5681ae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer Test Passed: Connected successfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 34\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# test_producer()\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     consumer \u001b[38;5;241m=\u001b[39m test_consumer()\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m         result \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/group.py:1197\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v2()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/group.py:1205\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/group.py:1120\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1119\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1120\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39mtimeout_ms, update_offsets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1126\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/group.py:657\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    655\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m--> 657\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_once(remaining, max_records, update_offsets\u001b[38;5;241m=\u001b[39mupdate_offsets)\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/group.py:684\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# Fetch positions if we have partitions we're subscribed to that we\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;66;03m# don't know the offset for\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription\u001b[38;5;241m.\u001b[39mhas_all_fetch_positions():\n\u001b[0;32m--> 684\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fetch_positions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription\u001b[38;5;241m.\u001b[39mmissing_fetch_positions())\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# If data is available already, e.g. from a previous network client\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# poll() call to commit, then just return it immediately\u001b[39;00m\n\u001b[1;32m    688\u001b[0m records, partial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetcher\u001b[38;5;241m.\u001b[39mfetched_records(max_records, update_offsets\u001b[38;5;241m=\u001b[39mupdate_offsets)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._update_fetch_positions\u001b[0;34m(self, partitions)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mrefresh_committed_offsets_if_needed()\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# Then, do any offset lookups in case some positions are not known\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetcher\u001b[38;5;241m.\u001b[39mupdate_fetch_positions(partitions)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/fetcher.py:186\u001b[0m, in \u001b[0;36mFetcher.update_fetch_positions\u001b[0;34m(self, partitions)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscriptions\u001b[38;5;241m.\u001b[39massignment[tp]\u001b[38;5;241m.\u001b[39mcommitted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# there's no committed position, so we need to reset with the\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# default strategy\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscriptions\u001b[38;5;241m.\u001b[39mneed_offset_reset(tp)\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_offset(tp)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     committed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscriptions\u001b[38;5;241m.\u001b[39massignment[tp]\u001b[38;5;241m.\u001b[39mcommitted\u001b[38;5;241m.\u001b[39moffset\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/fetcher.py:237\u001b[0m, in \u001b[0;36mFetcher._reset_offset\u001b[0;34m(self, partition)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoOffsetForPartitionError(partition)\n\u001b[1;32m    235\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResetting offset for partition \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m offset.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    236\u001b[0m           partition, strategy)\n\u001b[0;32m--> 237\u001b[0m offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve_offsets({partition: timestamp})\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partition \u001b[38;5;129;01min\u001b[39;00m offsets:\n\u001b[1;32m    240\u001b[0m     offset \u001b[38;5;241m=\u001b[39m offsets[partition][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kafka/consumer/fetcher.py:302\u001b[0m, in \u001b[0;36mFetcher._retrieve_offsets\u001b[0;34m(self, timestamps, timeout_ms)\u001b[0m\n\u001b[1;32m    300\u001b[0m             timestamps\u001b[38;5;241m.\u001b[39mpop(unknown_partition)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.0\u001b[39m)\n\u001b[1;32m    304\u001b[0m elapsed_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    305\u001b[0m remaining_ms \u001b[38;5;241m=\u001b[39m timeout_ms \u001b[38;5;241m-\u001b[39m elapsed_ms\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "KAFKA_BROKER = \"127.0.0.1:9092\"  # Change this if your Kafka runs on a different host/IP\n",
    "\n",
    "\n",
    "\n",
    "# Function to test Kafka Producer\n",
    "\n",
    "\n",
    "# Function to test Kafka Consumer\n",
    "def test_consumer():\n",
    "    try:\n",
    "        consumer = KafkaConsumer(\n",
    "            \"result\",  # Test topic\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            auto_offset_reset=\"earliest\",\n",
    "            enable_auto_commit=True,\n",
    "            consumer_timeout_ms=5000  # Wait for messages for 5 seconds\n",
    "        )\n",
    "        print(\"Consumer Test Passed: Connected successfully.\")\n",
    "        return consumer\n",
    "    except KafkaError as e:\n",
    "        print(\"Kafka Consumer Failed:\", e)\n",
    "        return False\n",
    "\n",
    "# Run all tests\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # test_producer()\n",
    "    consumer = test_consumer()\n",
    "\n",
    "\n",
    "    for message in consumer:\n",
    "\n",
    "        print(f\"Processing message: {message.value}\")\n",
    "        result = message.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0310dbfe-1369-4008-9c92-c65eb3cb985f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29262039-efb2-491f-ba34-e5e213f66a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e4e5c243-2a0b-40a1-909b-65869670fbf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error 61 connecting to localhost:6379. Connection refused.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/connection.py:357\u001b[0m, in \u001b[0;36mAbstractConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(), \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisconnect(error)\n\u001b[1;32m    359\u001b[0m     )\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/retry.py:62\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[0;34m(self, do, fail)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/connection.py:358\u001b[0m, in \u001b[0;36mAbstractConnection.connect.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[0;32m--> 358\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(), \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisconnect(error)\n\u001b[1;32m    359\u001b[0m     )\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/connection.py:730\u001b[0m, in \u001b[0;36mConnection._connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket.getaddrinfo returned an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/connection.py:718\u001b[0m, in \u001b[0;36mConnection._connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# connect\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(socket_address)\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# set the socket_timeout now that we're connected\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[220], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Fetch and display data for each key\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys_to_check:\n\u001b[0;32m---> 21\u001b[0m     value \u001b[38;5;241m=\u001b[39m redis_client\u001b[38;5;241m.\u001b[39mhget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m,key)  \u001b[38;5;66;03m# Try to get the key value\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/commands/core.py:4970\u001b[0m, in \u001b[0;36mHashCommands.hget\u001b[0;34m(self, name, key)\u001b[0m\n\u001b[1;32m   4962\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhget\u001b[39m(\n\u001b[1;32m   4963\u001b[0m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m   4964\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Awaitable[Optional[\u001b[38;5;28mstr\u001b[39m]], Optional[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m   4965\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4966\u001b[0m \u001b[38;5;124;03m    Return the value of ``key`` within the hash ``name``\u001b[39;00m\n\u001b[1;32m   4967\u001b[0m \n\u001b[1;32m   4968\u001b[0m \u001b[38;5;124;03m    For more information see https://redis.io/commands/hget\u001b[39;00m\n\u001b[1;32m   4969\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_command(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, key, keys\u001b[38;5;241m=\u001b[39m[name])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/client.py:559\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_command(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/client.py:565\u001b[0m, in \u001b[0;36mRedis._execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    563\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_pool\n\u001b[1;32m    564\u001b[0m command_name \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 565\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mget_connection(command_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_command_parse_response(\n\u001b[1;32m    569\u001b[0m             conn, command_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m    570\u001b[0m         ),\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[1;32m    572\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/connection.py:1422\u001b[0m, in \u001b[0;36mConnectionPool.get_connection\u001b[0;34m(self, command_name, *keys, **options)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_use_connections\u001b[38;5;241m.\u001b[39madd(connection)\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;66;03m# ensure this connection is connected to Redis\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;66;03m# connections that the pool provides should be ready to send\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;66;03m# a command. if not, the connection was either returned to the\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;66;03m# pool before all data has been read or the socket has been\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;66;03m# closed. either way, reconnect and verify everything is good.\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/redis/connection.py:363\u001b[0m, in \u001b[0;36mAbstractConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeout connecting to server\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message(e))\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock \u001b[38;5;241m=\u001b[39m sock\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mConnectionError\u001b[0m: Error 61 connecting to localhost:6379. Connection refused."
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "# List of keys to check\n",
    "keys_to_check = [\n",
    "    # \"active_sessions:6fac2199-edda-4a11-b209-125197a3140b:jobs:e8ede610-5a27-423e-956e-b8fb11215837:subtasks:e8ede610-5a27-423e-956e-b8fb11215837-subtask-4\",\n",
    "    # \"active_sessions:6fac2199-edda-4a11-b209-125197a3140b:jobs:e8ede610-5a27-423e-956e-b8fb11215837:subtasks\",\n",
    "    # \"active_sessions:6fac2199-edda-4a11-b209-125197a3140b:jobs:e8ede610-5a27-423e-956e-b8fb11215837:pending_subtasks\",\n",
    "    # \"jobs:e8ede610-5a27-423e-956e-b8fb11215837:completion\",\n",
    "    # \"jobs:e8ede610-5a27-423e-956e-b8fb11215837:result\",\n",
    "    # \"jobs:e8ede610-5a27-423e-956e-b8fb11215837:status\",\n",
    "    \"active_sessions:6fac2199-edda-4a11-b209-125197a3140b:jobs:e8ede610-5a27-423e-956e-b8fb11215837\"\n",
    "\n",
    "]\n",
    "\n",
    "# Fetch and display data for each key\n",
    "for key in keys_to_check:\n",
    "    value = redis_client.hget(\"status\",key)  # Try to get the key value\n",
    "    if value:\n",
    "        try:\n",
    "            value = json.loads(value)  # Attempt to parse JSON if applicable\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # Keep value as is if it's not JSON\n",
    "\n",
    "        print(f\"Key: {key}\\nValue: {value}\\n{'='*50}\")\n",
    "    else:\n",
    "        print(f\"Key: {key} does not exist or has no value.\\n{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2792b-d35d-4e74-81a6-3cc564c56a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
